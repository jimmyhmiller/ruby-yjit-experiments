use std::{mem, rc::Rc};

use crate::{
    backend::ir::Assembler,
    bbv::remove_block_version,
    branch::{regenerate_branch, set_branch_target},
    codegen::globals::CodegenGlobals,
    core::verify_blockid,
    cruby::rb_iseq_reset_jit_func,
    dev::stats::incr_counter,
    iseq::get_iseq_payload,
    meta::{
        block::{BlockRef, BranchShape, BranchStub, BranchTarget},
        invariants::block_assumptions_free,
    },
};

fn remove_from_graph(blockref: &BlockRef) {
    let block = blockref.borrow();

    // Remove this block from the predecessor's targets
    for pred_branchref in &block.incoming {
        // Branch from the predecessor to us
        let mut pred_branch = pred_branchref.borrow_mut();

        // If this is us, nullify the target block
        for target_idx in 0..=1 {
            if let Some(target) = pred_branch.targets[target_idx].as_ref() {
                if target.get_block().as_ref() == Some(blockref) {
                    pred_branch.targets[target_idx] = None;
                }
            }
        }
    }

    // For each outgoing branch
    for out_branchref in block.outgoing.iter() {
        let out_branch = out_branchref.borrow();

        // For each successor block
        for out_target in out_branch.targets.iter().flatten() {
            if let Some(succ_blockref) = &out_target.get_block() {
                // Remove outgoing branch from the successor's incoming list
                let mut succ_block = succ_blockref.borrow_mut();
                succ_block
                    .incoming
                    .retain(|succ_incoming| !Rc::ptr_eq(succ_incoming, out_branchref));
            }
        }
    }
}

/// Remove most references to a block to deallocate it.
/// Does not touch references from iseq payloads.
pub fn free_block(blockref: &BlockRef) {
    block_assumptions_free(blockref);

    remove_from_graph(blockref);

    // Branches have a Rc pointing at the block housing them.
    // Break the cycle.
    blockref.borrow_mut().incoming.clear();
    blockref.borrow_mut().outgoing = Box::new([]);

    // No explicit deallocation here as blocks are ref-counted.
}

// Invalidate one specific block version
pub fn invalidate_block_version(blockref: &BlockRef) {
    //ASSERT_vm_locking();

    // TODO: want to assert that all other ractors are stopped here. Can't patch
    // machine code that some other thread is running.

    let block = blockref.borrow();
    let cb = CodegenGlobals::get_inline_cb();

    verify_blockid(block.blockid);

    #[cfg(feature = "disasm")]
    {
        use crate::dev::options::get_option_ref;
        use crate::utils::iseq_get_location;
        // If dump_iseq_disasm is specified, print to console that blocks for matching ISEQ names were invalidated.
        if let Some(substr) = get_option_ref!(dump_iseq_disasm).as_ref() {
            let blockid_idx = block.blockid.idx;
            let iseq_location = iseq_get_location(block.blockid.iseq, blockid_idx);
            if iseq_location.contains(substr) {
                println!(
                    "Invalidating block from {}, ISEQ offsets [{}, {})",
                    iseq_location, blockid_idx, block.end_idx
                );
            }
        }
    }

    // Remove this block from the version array
    remove_block_version(blockref);

    // Get a pointer to the generated code for this block
    let block_start = block.start_addr;

    // Make the the start of the block do an exit. This handles OOM situations
    // and some cases where we can't efficiently patch incoming branches.
    // Do this first, since in case there is a fallthrough branch into this
    // block, the patching loop below can overwrite the start of the block.
    // In those situations, there is hopefully no jumps to the start of the block
    // after patching as the start of the block would be in the middle of something
    // generated by branch_t::gen_fn.
    let block_entry_exit = block
        .entry_exit
        .expect("invalidation needs the entry_exit field");
    {
        let block_end = block
            .end_addr
            .expect("invalidation needs constructed block");

        if block_start == block_entry_exit {
            // Some blocks exit on entry. Patching a jump to the entry at the
            // entry makes an infinite loop.
        } else {
            // Patch in a jump to block.entry_exit.

            let cur_pos = cb.get_write_ptr();
            let cur_dropped_bytes = cb.has_dropped_bytes();
            cb.set_write_ptr(block_start);

            let mut asm = Assembler::new();
            asm.jmp(block_entry_exit.as_side_exit());
            cb.set_dropped_bytes(false);
            asm.compile(cb);

            assert!(
                cb.get_write_ptr() <= block_end,
                "invalidation wrote past end of block (code_size: {:?}, new_size: {})",
                block.code_size(),
                cb.get_write_ptr().into_i64() - block_start.into_i64(),
            );
            cb.set_write_ptr(cur_pos);
            cb.set_dropped_bytes(cur_dropped_bytes);
        }
    }

    // For each incoming branch
    mem::drop(block); // end borrow: regenerate_branch might mut borrow this
    let block = blockref.borrow().clone();
    for branchref in &block.incoming {
        let mut branch = branchref.borrow_mut();

        let target_idx = if branch.get_target_address(0) == Some(block_start) {
            0
        } else {
            1
        };

        // Assert that the incoming branch indeed points to the block being invalidated
        let incoming_target = branch.targets[target_idx].as_ref().unwrap();
        assert_eq!(Some(block_start), incoming_target.get_address());
        if let Some(incoming_block) = &incoming_target.get_block() {
            assert_eq!(blockref, incoming_block);
        }

        CodegenGlobals::with_outlined_cb(|ocb| {
            // Create a stub for this branch target or rewire it to a valid block
            set_branch_target(
                target_idx as u32,
                block.blockid,
                &block.ctx,
                branchref,
                &mut branch,
                ocb,
            );
        });

        if branch.targets[target_idx].is_none() {
            // We were unable to generate a stub (e.g. OOM). Use the block's
            // exit instead of a stub for the block. It's important that we
            // still patch the branch in this situation so stubs are unique
            // to branches. Think about what could go wrong if we run out of
            // memory in the middle of this loop.
            branch.targets[target_idx] = Some(Box::new(BranchTarget::Stub(Box::new(BranchStub {
                address: block.entry_exit,
                id: block.blockid,
                ctx: block.ctx.clone(),
            }))));
        }

        // Check if the invalidated block immediately follows
        let target_next = Some(block.start_addr) == branch.end_addr;

        if target_next {
            // The new block will no longer be adjacent.
            // Note that we could be enlarging the branch and writing into the
            // start of the block being invalidated.
            branch.gen_fn.set_shape(BranchShape::Default);
        }

        // Rewrite the branch with the new jump target address
        let old_branch_size = branch.code_size();
        regenerate_branch(cb, &mut branch);

        if target_next && branch.end_addr > block.end_addr {
            panic!("yjit invalidate rewrote branch past end of invalidated block: {:?} (code_size: {})", branch, block.code_size());
        }
        if !target_next && branch.code_size() > old_branch_size {
            panic!(
                "invalidated branch grew in size (start_addr: {:?}, old_size: {}, new_size: {})",
                branch.start_addr.unwrap().raw_ptr(),
                old_branch_size,
                branch.code_size()
            );
        }
    }

    // Clear out the JIT func so that we can recompile later and so the
    // interpreter will run the iseq.
    //
    // Only clear the jit_func when we're invalidating the JIT entry block.
    // We only support compiling iseqs from index 0 right now.  So entry
    // points will always have an instruction index of 0.  We'll need to
    // change this in the future when we support optional parameters because
    // they enter the function with a non-zero PC
    if block.blockid.idx == 0 {
        // TODO:
        // We could reset the exec counter to zero in rb_iseq_reset_jit_func()
        // so that we eventually compile a new entry point when useful
        unsafe { rb_iseq_reset_jit_func(block.blockid.iseq) };
    }

    // FIXME:
    // Call continuation addresses on the stack can also be atomically replaced by jumps going to the stub.

    delayed_deallocation(blockref);

    CodegenGlobals::with_outlined_cb(|ocb| {
        ocb.unwrap().mark_all_executable();
    });
    cb.mark_all_executable();

    incr_counter!(invalidation_count);
}

// We cannot deallocate blocks immediately after invalidation since there
// could be stubs waiting to access branch pointers. Return stubs can do
// this since patching the code for setting up return addresses does not
// affect old return addresses that are already set up to use potentially
// invalidated branch pointers. Example:
//   def foo(n)
//     if n == 2
//       return 1.times { Object.define_method(:foo) {} }
//     end
//
//     foo(n + 1)
//   end
//   p foo(1)
pub fn delayed_deallocation(blockref: &BlockRef) {
    block_assumptions_free(blockref);

    // We do this another time when we deem that it's safe
    // to deallocate in case there is another Ractor waiting to acquire the
    // VM lock inside branch_stub_hit().
    remove_from_graph(blockref);

    let payload = get_iseq_payload(blockref.borrow().blockid.iseq).unwrap();
    payload.dead_blocks.push(blockref.clone());
}
